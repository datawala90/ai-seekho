{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35a19eab-1322-49c3-8030-07c18f4cb0c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T23:52:46.297760Z",
     "iopub.status.busy": "2025-07-06T23:52:46.297238Z",
     "iopub.status.idle": "2025-07-06T23:52:47.045661Z",
     "shell.execute_reply": "2025-07-06T23:52:47.044763Z",
     "shell.execute_reply.started": "2025-07-06T23:52:46.297731Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded chunk 5/8\n",
      "Embedded chunk 8/8\n",
      "✅ Built FAISS index with 8 vectors\n"
     ]
    }
   ],
   "source": [
    "import boto3, json, numpy as np\n",
    "import faiss\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 1) Your pancake text (again)\n",
    "doc_text = \"\"\"\n",
    "How to Make Simple Pancakes:\n",
    "\n",
    "Ingredients:\n",
    "– 1 cup all-purpose flour  \n",
    "– 2 tablespoons sugar  \n",
    "– 1 teaspoon baking powder  \n",
    "– 1 teaspoon baking soda  \n",
    "– 1 pinch of salt  \n",
    "– 1 cup buttermilk  \n",
    "– 1 egg  \n",
    "– 2 tablespoons melted butter  \n",
    "\n",
    "Instructions:\n",
    "In a bowl, whisk together the flour, sugar, baking powder, baking soda, and salt.  \n",
    "In another bowl, beat the egg with the buttermilk and melted butter until combined.  \n",
    "Pour the wet ingredients into the dry ingredients and stir until just mixed—it's okay if there are a few lumps.  \n",
    "\n",
    "Cooking:\n",
    "Heat a non-stick skillet over medium heat and lightly grease with butter.  \n",
    "Pour ¼ cup batter per pancake; cook until bubbles form on top (about 2 minutes), then flip and cook the other side until golden brown.  \n",
    "Serve warm with maple syrup, fresh fruit, or your favorite toppings.\n",
    "\"\"\"\n",
    "\n",
    "# 2) Chunk into ~200-char slices with 50-char overlap\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\",\" \\n\",\" \",\"\"],\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "chunks = splitter.split_text(doc_text)\n",
    "\n",
    "# 3) Bedrock runtime client\n",
    "rt = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "# 4) Embed one chunk at a time (using inputText)\n",
    "vectors = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    resp = rt.invoke_model(\n",
    "      modelId=EMBED_MODEL_ID,\n",
    "      contentType=\"application/json\",\n",
    "      accept=\"application/json\",\n",
    "      body=json.dumps({\"inputText\": chunk})\n",
    "    )\n",
    "    data = json.loads(resp[\"body\"].read())\n",
    "    vectors.append(data[\"embedding\"])\n",
    "    # optional progress print\n",
    "    if (i+1) % 5 == 0 or i==len(chunks)-1:\n",
    "        print(f\"Embedded chunk {i+1}/{len(chunks)}\")\n",
    "\n",
    "# 5) Normalize & index in FAISS\n",
    "arr = np.array(vectors, dtype=\"float32\")\n",
    "arr /= np.linalg.norm(arr, axis=1, keepdims=True)\n",
    "\n",
    "index = faiss.IndexFlatIP(arr.shape[1])\n",
    "index.add(arr)\n",
    "\n",
    "print(\"✅ Built FAISS index with\", index.ntotal, \"vectors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aaf25aa5-da0c-4023-87f2-90933667d3af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T23:52:47.046992Z",
     "iopub.status.busy": "2025-07-06T23:52:47.046765Z",
     "iopub.status.idle": "2025-07-06T23:52:47.051890Z",
     "shell.execute_reply": "2025-07-06T23:52:47.051293Z",
     "shell.execute_reply.started": "2025-07-06T23:52:47.046972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔑 Using TEXT_MODEL_ID = amazon.titan-text-express-v1\n"
     ]
    }
   ],
   "source": [
    "# ─── Cell A ───\n",
    "# Replace with the model you see “Access granted” for in your Base models list\n",
    "TEXT_MODEL_ID = \"amazon.titan-text-express-v1\"\n",
    "print(\"🔑 Using TEXT_MODEL_ID =\", TEXT_MODEL_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b757e67c-7a6c-48bf-a7b8-edd9faf2325a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T23:52:47.054001Z",
     "iopub.status.busy": "2025-07-06T23:52:47.053436Z",
     "iopub.status.idle": "2025-07-06T23:52:50.693919Z",
     "shell.execute_reply": "2025-07-06T23:52:50.693278Z",
     "shell.execute_reply.started": "2025-07-06T23:52:47.053978Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Q: What dry ingredients do I need?\n",
      "🟢 A: The dry ingredients you need are:\n",
      "1 cup all-purpose flour\n",
      "2 tablespoons sugar\n",
      "1 teaspoon baking powder\n",
      "1 teaspoon baking soda\n",
      "1 pinch of salt\n",
      "\n",
      "🔎 Q: How long should I cook each pancake side?\n",
      "🟢 A: Two minutes\n",
      "\n",
      "🔎 Q: What can I serve with the pancakes?\n",
      "🟢 A: The model cannot find sufficient information to answer the question.\n"
     ]
    }
   ],
   "source": [
    "import json, numpy as np\n",
    "\n",
    "# Make sure you have already set:\n",
    "#   EMBED_MODEL_ID = \"amazon.titan-embed-g1-text-02\"\n",
    "#   TEXT_MODEL_ID = \"amazon.titan-text-express-v1\"  # the Express G1 you opted into\n",
    "\n",
    "# 1) Retrieval helper—unchanged\n",
    "def retrieve(question: str, k: int = 2) -> list[str]:\n",
    "    resp = rt.invoke_model(\n",
    "        modelId=EMBED_MODEL_ID,\n",
    "        contentType=\"application/json\",\n",
    "        accept=\"application/json\",\n",
    "        body=json.dumps({\"inputText\": question})\n",
    "    )\n",
    "    vec = np.array(json.loads(resp[\"body\"].read())[\"embedding\"], dtype=\"float32\")\n",
    "    vec /= np.linalg.norm(vec)\n",
    "    _, ids = index.search(vec.reshape(1, -1), k)\n",
    "    return [chunks[i] for i in ids[0]]\n",
    "\n",
    "# 2) RAG + Titan-Text-Express invoke\n",
    "def rag(question: str, k: int = 2) -> str:\n",
    "    ctx    = \"\\n\\n---\\n\\n\".join(retrieve(question, k))\n",
    "    prompt = f\"Context:\\n{ctx}\\n\\nQuestion: {question}\"\n",
    "\n",
    "    payload = {\n",
    "      \"inputText\": prompt,\n",
    "      \"textGenerationConfig\": {\n",
    "         \"temperature\": 0.0,\n",
    "         \"topP\": 1.0,\n",
    "         \"maxTokenCount\": 256    # omit stopSequences entirely\n",
    "      }\n",
    "    }\n",
    "\n",
    "    resp = rt.invoke_model(\n",
    "        modelId=TEXT_MODEL_ID,\n",
    "        contentType=\"application/json\",\n",
    "        accept=\"application/json\",\n",
    "        body=json.dumps(payload)\n",
    "    )\n",
    "    result = json.loads(resp[\"body\"].read())\n",
    "    return result[\"results\"][0][\"outputText\"].strip()\n",
    "\n",
    "# 3) Test\n",
    "for q in [\n",
    "    \"What dry ingredients do I need?\",\n",
    "    \"How long should I cook each pancake side?\",\n",
    "    \"What can I serve with the pancakes?\"\n",
    "]:\n",
    "    print(f\"\\n🔎 Q: {q}\\n🟢 A: {rag(q)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d18d48-7b85-4311-b789-239283414da9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efffaeb-6aff-4303-8c1c-db5cdc8c1ce5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
